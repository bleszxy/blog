[TOC]

## CAP理论

CAP 理论对分布式系统的特性做了高度抽象，形成如下三个指标：

1. 一致性(Consistency) 
2. 可用性(Availability) 
3. 分区容错性(Partition Tolerance)

### 指标描述

#### 一致性

一致性说的是客户端的每次读操作，不管访问哪个节点，要么读到的都是同一份最新的数据，要么读取失败。 **一致性强调的不是数据完整，而是各节点间的数据一致。**

#### 可用性

集群毕竟不是单机，当发生分区故障的时候，有时不能仅仅因为节点间出现了通讯问题，节点中的数据会不一致，就拒绝写入新数据，之后在客户端查询数据时，就一直返回给客户端出错信息。 

场景举例：业务集群中的一些关键系统，比如名字路由系统，如果仅仅因为发生了分布故障，节点中的数据会不一致，集群就拒绝写入新的路由信息，之后，当客户端查询相关路由信息时，系统就一直返回给客户端出错信息，那么相关的服务都将因为获取不到指定路由信息而不可用、瘫痪，这可以说是灾难性的故障了。

**这个指标强调的是服务可用，但不保证数据的一致性**（我尽力给你返回数据，不会不响应你，但是我不保证集群内每个节点给你的数据都是最新的）。 

#### 分区容错性

当节点间出现任意数量的消息丢失或高延迟的时候，系统仍然可以继续提供服务。也就是说，分布式系统在告诉访问本系统的客户端:不管我的内部出现什么样的数据同步问题，我会一直运行，提供服务。这个指标，**强调的是集群对分区故障的容错能力**。



### CAP不能三角

CAP 不可能三角说的是对于一个分布式系统而言，3个指标不能同时兼得，只能在3个指标中选2个。

<img src="all_images/image-20230216193340079.png" width=70% height=70% />

### CAP如何选择？

只要有网络交互就一定会有延迟和数据丢失，而这种状况我们必须接受，还必须保证系统不能挂掉，节点间的分区故障是必然发生的，分区容错性(P)是前提，是必须要保证的，最终只能选择CP或AP。一致性(C)和可用性(A)，要么选择一致性，保证数据绝对一致;要么选择可用性，保证服务可用。

#### CP

当选择了一致性(C)的时候，如果因为消息丢失、延迟过高发生了网络分区，部分节点无法保证特定信息是最新的，那么这个时候，当集群节点接收到来自客户端的写请求时，因为无法保证所有节点都是最新信息，所以系统将返回写失败错误，也就是说集群拒绝新数据写入。

#### AP

当选择了可用性(A)的时候，系统将始终处理客户端的查询，返回特定信息，如果发生了网络分区，一些节点将无法返回最新的特定信息，它们将返回自己当前的相对新的信息。



CP 模型：典型的应用是 ZooKeeper，Etcd 和 HBase。
AP 模型：Cassandra 和 DynamoDB。



## BASE理论

BASE理论是 **CAP** 理论中的 **AP** 的延伸，是对**互联网大规模分布式系统的实践总结，强调可用性**。几乎所有的互联网后台分布式系统都有 BASE 的支持，这个理论很重要，地位也很高。一旦掌握它，你就能掌握绝大部分场景的分布式系统的架构技巧，设计出适合业务场景特点的、高可用性的分布式系统。BASE理论的核心就是基本可用和最终一致性。



**基本可用(Basically Available)**

基本可用是说，当分布式系统在出现不可预知的故障时，允许损失部分功能的可用性，保障核心功能的可用性。就像弹簧一样，遇到外界的压迫，它不是折断，而是变形伸缩，不断适应外力，实现基本的可用。实现基本可用的4板斧：

**1、服务降级**

牺牲部分功能的可用性，保障系统的核心功能可用

例如：体验降级， 比如用小图片来替代原始图片，通过降低图片的清晰度和大小，提升系统的处理能力。

**2、流量削峰**

将访问请求错开，削弱请求峰值

例如：12306深圳出发的火车票在**8点**开售，北京出发的火车票在**9点**开售。

**3、延迟响应**

超出系统处理能力的突发流量情况下，牺牲响应时间的可用性，保障核心功能运行

例如：12306春运期间，自己提交的购票请求，往往会在队列中排队等待处理，可能几分钟或十几分钟后，系统才开始处理，然后响应处理结果。

**4、过载保护**

​        对自身系统可承受的流量有预期，超过后排队或者拒绝

 比如：把接收到的请求放在指定的队列中排队处理，如果请求等待时间超时了(假设是 100ms)，这个时候直接拒绝超时请求；再比如队列满了之后，就清除队列中一定数量的排队请求，保护系统不过载，实现系统的基本可用。



**最终一致性(Eventually consistent)**

最终一致性是说，系统中**所有的数据副本在经过一段时间的同步后，最终能够达到一个一致的状态**。也就是说，在数据一致性上，存在一个短暂的延迟。

几乎所有的互联网系统采用的都是最终一致性，只有在实在无法使用最终一致性，才使用强一致性或事务，比如，对于决定系统运行的敏感元数据，需要考虑采用强一致性，对于与钱有关的支付系统或金融系统的数据，需要考虑采用事务。

**最终一致性常用实现方式**

**1、读时修复**

在读取数据时，检测数据的不一致，进行修复。比如 Cassandra 的 Read Repair 实现，具体来说，在向 Cassandra 系统查询数据的时候，如果检测到不同节点 的副本数据不一致，系统就自动修复数据。

**2、写时修复**

在写入数据，检测数据的不一致时，进行修复。比如 Cassandra 的 Hinted Handoff 实现。具体来说，Cassandra 集群的节点之间远程写数据的时候，如果写失败就将数据缓存下来，然后定时重传，修复数据的不一致性。

**3、异步修复 -> 计算节点锁定状态修复**

定时对账检测副本数据的一致性，并修复。

**注意**

1、写时修复不需要做数据一致性对比，性能消耗比较低，对系统运行影响也不大，所以我推荐你在实现最终一致性时优先实现这种方式。而读时修复和异步修复因为需要做数据的一致性对比，性能消耗比较多，在开发实际系统时，你要尽量优化一致性对比的算法，降低性能消耗，避免对系统运行造成影响。

2、在实现最终一致性的时候，我推荐同时实现自定义写一致性级别 (All、Quorum、One、Any)， 让用户可以自主选择相应的一致性级别，比如可以通过设置一致性级别为 All，来实现强一致性。



**理论实践**

DATA 节点的核心功能是读和写，所以基本可用是指读和写的基本可用。那么我们可以通 过**分片**和**多副本**，实现读和写的基本可用。也就是说，**将同一业务的数据先分片，然后再以多份副本的形式分布在不同的节点上**。比如下面这张图，这个 3 节点 2 副本的集群，除非 **超过一半的节点都故障了，否则是能保障所有数据的读写的**。

<img src="all_images/image-20230215112101127.png" width=70% height=70% />







**其他**

**小结**

1. BASE 理论是对 CAP 中一致性和可用性权衡的结果，它来源于对大规模互联网分布式系 统实践的总结，是基于 CAP 定理逐步演化而来的。它的核心思想是，如果不是必须的 话，不推荐实现事务或强一致性，鼓励可用性和性能优先，根据业务的场景特点，来实 现非常弹性的基本可用，以及实现数据的最终一致性。

2. BASE 理论主张通过牺牲部分功能的可用性，实现整体的基本可用，也就是说，通过服务 降级的方式，努力保障极端情况下的系统可用性。

3. ACID 理论是传统数据库常用的设计理念，追求强一致性模型。BASE 理论支持的是大型 分布式系统，通过牺牲强一致性获得高可用性。BASE 理论在很大程度上，解决了事务型 系统在性能、容错、可用性等方面痛点。**BASE 理论在 NoSQL 中应 用广泛，是 NoSQL 系统设计的事实上的理论支撑**。



**软状态**

软状态描述的是实现服务**可用性**的时候系统数据的一种过渡状态，也就是说不同节点间，数据副本存在短暂的不一致。



**强一致性必然影响可用性**

比如：假设 3 个节点的集群，每个节点的 可用性为 99.9%，那么整个集群的可用性为 99.7%。

解决可用性低的关键在于根据实际场景，尽量采用可用性优先的**AP**模型。







## Paxos算法(paikesasi)



在过去几十年里，Paxos算法基本上是分布式共识的代名词，由（兰伯特）提出，当前最常用的一批共识算法都是基于它改进的。比如，Fast Paxos 算法、 Cheap Paxos 算法、Raft 算法、ZAB 协议等等。 Paxos 算法有一定的难度，可分布式算法本身就很复杂，Paxos 然也不会例外！ 



Paxos 算法包含 2 个部分:

1、**Basic Paxos** 算法，描述的是**多节点之间如何就某个值(提案 Value)达成共识**;

2、**Multi-Paxos** 思想，描述的是**执行多个 Basic Paxos 实例，就一系列值达成共识**。PS：兰伯特提到的 Multi-Paxos 思想，缺少代码实现的必要细节(比如怎么**选举领导者**)，所以在理解上比较难。



**Basic Paxos**

在 **Basic Paxos** 中，有**提议者(Proposer)**、**接受者(Acceptor)**、**学习者(Learner)** 三种角色，他们之间的关系如下:

<img src="all_images/image-20230215112254322.png" width=70% height=70% />





**提议者(Proposer)**

提议一个值，用于投票表决。



**接受者(Acceptor)**

对每个提议的值进行投票，并存储接受的值。一般来说，**集群中的所有节点都在扮演接受者的角色，参与共识协商，并接受和存储数据**。



**学习者(Learner)** 

被告知投票的结果，接受达成共识的值，存储保存，不参与投票的过程。一般来说，**学习者是数据备份节点**，比如“Master-Slave”模型中的 Slave，被动地接受数据，容灾备份。



**如何达成共识？**

Basic Paxos 是**通过二阶段提交的方式来达成共识的**。二阶段提交是达成共识的常用方式。

**1、准备(Prepare)阶段**

在准备请求中是不需要指定提议的值的，只需要携带提案编号就可以了。

注意：

如果准备请求的提案编号，小于等于接受者已经响应 的准备请求的提案编号，那么接受者将承诺不响应这个准备请求

**2、接受(Accept)阶段**

并发提议，会协商并接受编号最大的值。

注意：

如果接受请求中的提案的提案编号，小于接受者已经响应的准备请求的提案编号，那么接受者将承诺不通过这个提案

如果接受者之前有通过提案，那么接受者将承诺，会在准备请求的响应中， 包含已经通过的最大编号的提案信息。



**容错能力**

Basic Paxos 的容错能力，源自“大多数”的约定，你可以这么理解:**当少于一半的节点出现故障**的时候，共识协商仍然在正常工作。



**Multi-Paxos**

兰伯特提到的 Multi-Paxos 是一种思想，不是算法。而 Multi-Paxos 算法是一个统称，它是指基于 Multi-Paxos 思想，通过多个 Basic Paxos 实例实现一系列值的共识的算法(比如 Chubby 的 Multi-Paxos 实现、Raft 算法等)





**领导者（Leader）**

引入领导者节点，领导者节点作为唯一提议者，这样就不存在多个提议者同时提交提案的情况，也就不存在提案冲突的情况了（2 轮 RPC 通讯(准备阶段和接受阶段)往返消息多、耗性能、延迟大。）

<img src="all_images/image-20230215112333744.png" width=70% height=70% />



注意：在论文中，兰伯特没有说如何选举领导者，需要我们在实现 Multi- Paxos 算法的时候自己实现。 比如在 Chubby 中，主节点(也就是领导者节点)是通过执 行 Basic Paxos 算法，进行投票选举产生的。





**优化 Basic Paxos 执行**

“当领导者处于稳定状态时，省掉准备阶段，直接进入接受阶段”这个优化机 制，优化 Basic Paxos 执行。也就是说，领导者节点上，序列中的命令是最新的，不再需 要通过准备请求来发现之前被大多数节点通过的提案，领导者可以独立指定提案中的值。这 时，领导者在提交命令时，可以省掉准备阶段，直接进入到接受阶段：

<img src="all_images/image-20230215112427354.png" width=70% height=70% />





**Chubby 的 Multi-Paxos 实现**

1、通过引入**主节点**，实现了兰伯特提到的**领导者(Leader)节点特性**。也就是说，**主节点作为唯一提议者，这样就不存在多个提议者同时提交提案的情况，也就不存在提案冲突的情况了**。

2、主节点是通过执行 Basic Paxos 算法，进行投票选举产生的，并且在运行过程中，**主节点会通过不断续租的方式来延长租期(Lease)**。

3、实现了**Basic Paxos**优化，当领导者处于稳定状态时，省掉准备阶段，直接进入接受阶段。

4、实现了**成员变更(Group membership)**，以此保证节点变更的时 候集群的平稳运行。

补充：**Chubby**为了实现强一致性，读操作也只能在主节点上执行。



**Chubby写入流程**

当主节点从客户端接收到写请求后，作为提议者，执行 Basic Paxos实例，将数据发送给所有的节点，并且在大多数的服务器接受了这个写请求之后，再响应给客户端成功。

<img src="all_images/image-20230215112451820.png" width=70% height=70% />









**Chubby读取流程**

当主节点接收到读请求后，只需要查询本地数据，然后返回给客户端。

<img src="all_images/image-20230215112545119.png" width=70% height=70% />





**思考**

1**、**当**领导者处于稳定状态**时，**省掉准备阶段**，**直接进入接受阶段**”这个优化机制，是通过**减少非必须的协商步骤来提升性能**的。这种方法非常常用，也很有效。

2、**Chubby**读取和写入都在主节点，限制了集群处理写请求的并发能力，性能约等于单机（**写入达不到单机，需要多数节点响应**）。

2、**Chubby**也约定了大多数原则，Chubby能容错**(n-1)/2个节点**的**故障**。

4、疑问：**Chubby**是否所有节点都可以接收读写请求，非主节点收到请求后将请求转发给主节点？





## Raft 算法

Raft算法属于Multi-Paxos算法，它是在兰伯特 Multi-Paxos 思想的基础上，做了一些简化和限制，比如**增加了日志必须是连续的**，只支持**领导者**、**跟随者**和**候选人**三种状态，在理解和算法实现上都相对容易许多。

**Raft 算法是现在分布式系统开发首选的共识算法**，全新的系统大多选择了Raft 算法(比如 Etcd、Consul、CockroachDB、Tidb、OceanBase等)。

一句话概述：**从本质上说，Raft 算法是通过一切以领导者为准的方式，实现一系列值的共识和各节点日志的一致**。



**动态图**

[http://thesecretlivesofdata.com/raft/](http://thesecretlivesofdata.com/raft/)



**Raft成员身份**

Raft 算法支持**领导者(Leader)**、**跟随者 (Follower)**和**候选人(Candidate)** 3 种状态。

**Follower**

接收和处理来自Leader的消息，当等待Leader心跳信息超时的时候，就主动站出来，推荐自己当Candidate。

**Candidate**

向其他节点发送请求投票(RequestVote)RPC 消息，通知其他节点来投票，如果赢得了大多数选票，就晋升当leader

**Leader**

处理写请求、管理日志复制和不断地发送心跳信息，通知其他节点“我是leader，我还存活“。

**注意：Raft 算法是强领导者模型，集群中只能有一个Leader。**



**Leader选举过程**

1、初始状态下，集群中所有的节点都是Follower(跟随者)状态。

<img src="all_images/image-20230215112618584.png" width=70% height=70% >



2、Raft 算法实现了随机超时时间（150-300ms）的特性。也就是说，每个节点等待领导者节点心跳信息的超时时间间隔是随机的。集群中没有领导者，而节点 A 的等待 超时时间最小(150ms)，它会最先因为没有等到领导者的心跳信息，发生超时。这个时候，节点 A 就增加自己的任期编号，并推举自己为候选人，先给自己投上一张选票，然后向其他节点发送请求投票 RPC 消息，请它们选举自己为领导者。

<img src="all_images/image-20230215112703320.png" width=70% height=70% />



3、如果其他节点接收到候选人 A 的请求投票 RPC 消息，在编号为 1 的这届任期内，也还没有进行过投票，那么它将把选票投给节点 A，并增加自己的任期编号。

<img src="all_images/image-20230215112744987.png" width=70% height=70% />



4、 如果候选人在选举超时时间内赢得了大多数的选票，那么它就会成为本届任期内新的领导者。

<img src="all_images/image-20230215112811664.png" width=70% height=70% />



5、 节点 A 当选领导者后，他将周期性地发送心跳消息，通知其他服务器我是领导者，阻止跟随者发起新的选举、篡权。

<img src="all_images/image-20230215112842428.png" width=70% height=70% />





**注意**

领导者 需要处理来自客户的**写请求**，并通过**日志复制实现各节点日志的一致**。



**日志复制过程**

在 Raft 算法中，副本数据是以日志的形式存在的，**领导者接收到来自客户端写请求后，处理写请求的过程就是一个复制和提交日志项的过程**。



**什么是日志项？**

日志项是一种数据格式，它主要包含用户指定的数据，也就是指令(Command)， 还包含一些附加信息，比如索引值(Log index)、任期编号(Term)。

<img src="all_images/image-20230215112909320.png" width=70% height=70% />



**指令**

一条由客户端请求指定的、状态机需要执行的指令。你可以将指令理解成客户端指定的数据。

**索引值**

日志项对应的整数索引值。它其实就是用来标识日志项的，是一个连续的、单调递增的整数号码。

**任期编号**

创建这条日志项的领导者的任期编号。

注意：一届领导者任期，往往有多条日志项，而且日志项的索引值是连续的



**如何复制日志？**



<img src="all_images/image-20230215112944379.png" width=70% height=70% />



1. 接收到客户端请求后，领导者基于客户端请求中的指令，创建一个新日志项，并附加到本地日志中。

2. 领导者通过日志复制RPC，将新的日志项复制到其他的服务器。

3. 当领导者将日志项，成功复制到大多数的服务器上的时候，领导者会将这条日志项提交 到它的状态机中。

4. 领导者将执行的结果返回给客户端。

5. 当跟随者接收到心跳信息，或者新的日志复制 RPC 消息后，如果跟随者发现领导者已经提交了某条日志项，而它还没提交，那么跟随者就将这条日志项提交到本地的状态机中。 （注意：这一点是对二阶段提交的优化，成为一阶段提交，减少了一半的往返消息，也就是降低了一半的消息延迟）



**如何实现日志一致性？**

在 Raft 算法中，领导者通过**强制跟随者直接复制自己的日志项，处理不一致日志**。也就是说，**Raft 是通过以领导者的日志为准，来实现各节点日志的一致的**。



**PrevLogEntry**：表示当前要复制的日志项，前面一条日志项的索引值。

**PrevLogTerm**：表示当前要复制的日志项，前面一条日志项的任期编号。

<img src="all_images/image-20230215113025858.png" width=70% height=70% />





1. 领导者通过日志复制 RPC 消息，发送当前最新日志项到跟随者(为了演示方便，假设当前需要复制的日志项是最新的)，这个消息的 PrevLogEntry 值为 7，PrevLogTerm值为 4。

2. 如果跟随者在它的日志中，找不到与PrevLogEntry值为 7、PrevLogTerm值为 4 的日志项，也就是说它的日志和领导者的不一致了，那么跟随者就会拒绝接收新的日志项， 并返回失败信息给领导者。

3. 这时，领导者会递减要复制的日志项的索引值，并发送新的日志项到跟随者，这个消息的PrevLogEntry值为6，PrevLogTerm值为3。

4. 如果跟随者在它的日志中，找到了 PrevLogEntry 值为 6、PrevLogTerm 值为 3 的日志项，那么日志复制 RPC 返回成功，这样一来，领导者就知道在PrevLogEntry 值为 6, PrevLogTerm 值为 3 的位置，跟随者的日志项与自己相同。

5. 领导者通过日志复制 RPC，复制并更新覆盖该索引值之后的日志项(也就是不一致的日志项)，最终实现了集群各节点日志的一致。
